{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04dd15d7",
   "metadata": {},
   "source": [
    "## 构建RAG应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511c0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv \n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fefca3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7f6e416381c0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f6e2b46cfa0>, root_client=<openai.OpenAI object at 0x7f6e2b76ea70>, root_async_client=<openai.AsyncOpenAI object at 0x7f6e4163b730>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5467bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a893ca92-5ecf-42a8-a4b3-e34f0bec5ff1-0', usage_metadata={'input_tokens': 14, 'output_tokens': 7, 'total_tokens': 21})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm.invoke(\"What is the capital of France?\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4355b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里我们要求模型对给定文本进行中文翻译\n",
    "prompt = \"\"\"请你将由三个反引号分割的文本翻译成英文！\\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03be79e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'请你将由三个反引号分割的文本翻译成英文！text: ```我带着比身体重的行李，游入尼罗河底，经过几道闪电 看到一堆光圈，不确定是不是这里。```\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"我带着比身体重的行李，\\\n",
    "游入尼罗河底，\\\n",
    "经过几道闪电 看到一堆光圈，\\\n",
    "不确定是不是这里。\\\n",
    "\"\n",
    "prompt.format(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b610ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='你是一个翻译助手，可以帮助我将 中文 翻译成 英文.', additional_kwargs={}, response_metadata={}), HumanMessage(content='我带着比身体重的行李，游入尼罗河底，经过几道闪电 看到一堆光圈，不确定是不是这里。', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"你是一个翻译助手，可以帮助我将 {input_language} 翻译成 {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate([(\"system\", template), (\"human\", human_template)])\n",
    "\n",
    "text = \"我带着比身体重的行李，\\\n",
    "游入尼罗河底，\\\n",
    "经过几道闪电 看到一堆光圈，\\\n",
    "不确定是不是这里。\\\n",
    "\"\n",
    "messages = chat_prompt.invoke({\"input_language\": \"中文\", \"output_language\": \"英文\", \"text\": text})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a48b121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I carried luggage heavier than my body and dived into the bottom of the Nile River. After passing through several flashes of lightning, I saw a pile of halos, not sure if this is the place.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 95, 'total_tokens': 137, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b17de4a1-685a-4028-909e-fdf226023916-0', usage_metadata={'input_tokens': 95, 'output_tokens': 42, 'total_tokens': 137})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm.invoke(messages)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "354b5b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I carried luggage heavier than my body and dived into the bottom of the Nile River. After passing through several flashes of lightning, I saw a pile of halos, not sure if this is the place.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97834fb9",
   "metadata": {},
   "source": [
    "#### 组合成一个链条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab751a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | llm | output_parser # LCEL模板语法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa386de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我扛着比我的身体还重的行李，潜入了尼罗河的底部。穿过几道闪电后，我看到了一堆光环，不确定这是否就是目的地。'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I carried luggage heavier than my body and dived into the bottom of the Nile River. After passing through several flashes of lightning, I saw a pile of halos, not sure if this is the place.'\n",
    "chain.invoke({\"input_language\": \"英文\", \"output_language\": \"中文\",\"text\": text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27cb918",
   "metadata": {},
   "source": [
    "### 构建检索问答链"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad4e24",
   "metadata": {},
   "source": [
    "#### 加载向量数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6225503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./llm-universe/notebook/C4 构建 RAG 应用\")\n",
    "\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64e1e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30866c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18932/3977479060.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  embeddings = OpenAIEmbeddings()\n",
      "/tmp/ipykernel_18932/3977479060.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vector_db = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "persist_directory = './llm-universe/data_base/vector_db/chroma'\n",
    "\n",
    "vector_db = Chroma(\n",
    "  persist_directory=persist_directory,\n",
    "  embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e37281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量库中存储的数量：0\n"
     ]
    }
   ],
   "source": [
    "print(f\"向量库中存储的数量：{vector_db._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22857436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的内容数：0\n"
     ]
    }
   ],
   "source": [
    "question = \"什么是 prompt engineering？\"\n",
    "retriever = vector_db.as_retriever(search_keywords={'k': 3})\n",
    "docs = retriever.invoke(question)\n",
    "print(f\"检索到的内容数：{len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a50b75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n {doc.page_content}\", end=\"\\n-----------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbeed0",
   "metadata": {},
   "source": [
    "#### 创建检索链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24277ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def combine_docs(docs):\n",
    "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "combiner = RunnableLambda(combine_docs)\n",
    "retrieval_chain = retriever | combiner\n",
    "retrieval_chain.invoke(\"南瓜书是什么？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092dda96",
   "metadata": {},
   "source": [
    "#### 创建 LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7101b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b99e50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'当然可以！我是一个由OpenAI开发的人工智能助手，名叫ChatGPT。我擅长处理自然语言，可以帮助回答问题、提供信息、协助解决问题以及进行各种对话。我没有个人意识或情感，只是一个基于大量数据训练的语言模型，旨在为用户提供有用和准确的回答。如果你有任何问题或需要帮助，随时可以问我！'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0.0)\n",
    "\n",
    "llm.invoke('请你介绍一下你自己').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e3a974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答\n",
    "案。最多使用三句话。尽量使答案简明扼要。请你在回答的最后说“谢谢你的提问！”。\n",
    "{context}\n",
    "问题: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template)\n",
    "\n",
    "qa_chain = (RunnableParallel({'context': retrieval_chain, 'input': RunnablePassthrough()}) | prompt | llm | StrOutputParser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d145b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"什么是南瓜书？\"\n",
    "question_2 = \"Prompt Engineering for Developer是谁写的？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec061a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型+知识库后回答 question_1 的结果：\n",
      "南瓜书是对《深度学习》这本书的昵称，由Ian Goodfellow、Yoshua Bengio和Aaron Courville合著。由于书的封面是橙色的，形似南瓜，因此得名。该书是深度学习领域的重要教材。谢谢你的提问！\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain.invoke(question_1)\n",
    "print(\"大模型+知识库后回答 question_1 的结果：\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad5d3818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型+知识库后回答 question_2 的结果：\n",
      "根据提供的上下文，我不知道《Prompt Engineering for Developer》这本书是谁写的。谢谢你的提问！\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain.invoke(question_2)\n",
    "print(\"大模型+知识库后回答 question_2 的结果：\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "068c4ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"南瓜书\"是对《深度学习：算法与实现》的昵称，这本书由李沐、阿斯顿张、扎卡里·C·立顿和亚历山大·J·斯莫拉编写。之所以被称为\"南瓜书\"，是因为书的封面上有一个显眼的南瓜图案。这本书以其通俗易懂的语言和丰富的代码示例，广受深度学习初学者和实践者的欢迎。书中涵盖了深度学习的基本概念、常用算法以及如何使用深度学习框架进行实现。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(question_1).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14213675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'《Prompt Engineering for Developers》是由Isa Fulford和Andrew M. White撰写的。这本书旨在帮助开发者更好地理解和应用提示工程技术，以提高与大型语言模型的交互效果。'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(question_2).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5364c",
   "metadata": {},
   "source": [
    "#### 向向量添加聊天记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89aea37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"你是一个问答任务的助手。 \"\n",
    "    \"请使用检索到的上下文片段回答这个问题。 \"\n",
    "    \"如果你不知道答案就说不知道。 \"\n",
    "    \"请使用简洁的话语回答用户。\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate([('system', system_prompt), ('placeholder', '{chat_history}'), ('human', '{input}')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70a1e610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一个问答任务的助手。 请使用检索到的上下文片段回答这个问题。 如果你不知道答案就说不知道。 请使用简洁的话语回答用户。\n",
      "\n",
      "\n",
      "南瓜书是什么？\n"
     ]
    }
   ],
   "source": [
    "messages = qa_prompt.invoke({\n",
    "  'input': '南瓜书是什么？',\n",
    "  'chat_history': [],\n",
    "  'context': ''\n",
    "})\n",
    "\n",
    "for message in messages.messages:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1338be9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你是一个问答任务的助手。 请使用检索到的上下文片段回答这个问题。 如果你不知道答案就说不知道。 请使用简洁的话语回答用户。\n",
      "\n",
      "\n",
      "西瓜书是什么？\n",
      "西瓜书是指周志华老师的《机器学习》一书，是机器学习领域的经典入门教材之一。\n",
      "你可以介绍一下他吗？\n"
     ]
    }
   ],
   "source": [
    "messages = qa_prompt.invoke(\n",
    "    {\n",
    "        \"input\": \"你可以介绍一下他吗？\",\n",
    "        \"chat_history\": [\n",
    "            (\"human\", \"西瓜书是什么？\"),\n",
    "            (\"ai\", \"西瓜书是指周志华老师的《机器学习》一书，是机器学习领域的经典入门教材之一。\"),\n",
    "        ],\n",
    "        \"context\": \"\"\n",
    "    }\n",
    ")\n",
    "for message in messages.messages:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4fa8b",
   "metadata": {},
   "source": [
    "#### 带有信息压缩的检索链"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f1d333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "condense_question_system_template = (\n",
    "    \"请根据聊天记录完善用户最新的问题，\"\n",
    "    \"如果用户最新的问题不需要完善则返回用户的问题。\"\n",
    "    )\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate([\n",
    "  ('system', condense_question_system_template),\n",
    "  ('placeholder', '{chat_history}'),\n",
    "  ('human', '{input}')\n",
    "])\n",
    "\n",
    "retrieve_docs = RunnableBranch(\n",
    "    # 分支 1: 若聊天记录中没有 chat_history 则直接使用用户问题查询向量数据库\n",
    "    (lambda x: not x.get(\"chat_history\", False), (lambda x: x[\"input\"]) | retriever, ),\n",
    "    # 分支 2 : 若聊天记录中有 chat_history 则先让 llm 根据聊天记录完善问题再查询向量数据库\n",
    "    condense_question_prompt | llm | StrOutputParser() | retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1084d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新定义 combine_docs\n",
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs[\"context\"]) # 将 docs 改为 docs[\"context\"]\n",
    "# 定义问答链\n",
    "qa_chain = (\n",
    "    RunnablePassthrough.assign(context=combine_docs) # 使用 combine_docs 函数整合 qa_prompt 中的 context\n",
    "    | qa_prompt # 问答模板\n",
    "    | llm\n",
    "    | StrOutputParser() # 规定输出的格式为 str\n",
    ")\n",
    "# 定义带有历史记录的问答链\n",
    "qa_history_chain = RunnablePassthrough.assign(\n",
    "    context = (lambda x: x) | retrieve_docs # 将查询结果存为 content\n",
    "    ).assign(answer=qa_chain) # 将最终结果存为 answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c02b149a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '西瓜书是什么？',\n",
       " 'chat_history': [],\n",
       " 'context': [],\n",
       " 'answer': '\"西瓜书\"是对《机器学习》一书的昵称，由周志华教授编写。由于书的封面是绿色的，并且有一个西瓜的图案，因此被称为\"西瓜书\"。这本书是机器学习领域的一本重要教材，广泛用于教学和自学。'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不带聊天记录\n",
    "qa_history_chain.invoke({\n",
    "    \"input\": \"西瓜书是什么？\",\n",
    "    \"chat_history\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab46e583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '南瓜书跟它有什么关系？',\n",
       " 'chat_history': [('human', '西瓜书是什么？'),\n",
       "  ('ai', '西瓜书是指周志华老师的《机器学习》一书，是机器学习领域的经典入门教材之一。')],\n",
       " 'context': [],\n",
       " 'answer': '南瓜书是指李航老师的《统计学习方法》一书。与西瓜书一样，南瓜书也是机器学习领域的重要教材，但侧重于统计学习方法。两者都是学习机器学习的常用参考书籍。'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 带聊天记录\n",
    "qa_history_chain.invoke({\n",
    "    \"input\": \"南瓜书跟它有什么关系？\",\n",
    "    \"chat_history\": [\n",
    "        (\"human\", \"西瓜书是什么？\"),\n",
    "        (\"ai\", \"西瓜书是指周志华老师的《机器学习》一书，是机器学习领域的经典入门教材之一。\"),\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb662d8f",
   "metadata": {},
   "source": [
    "### 部署知识库助手"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c012448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch, RunnablePassthrough\n",
    "import sys\n",
    "sys.path.append(\"./llm-universe/notebook/C3 搭建知识库\") # 将父目录放入系统路径中\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8eda5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever():\n",
    "    # 定义 Embeddings\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    # 向量数据库持久化路径\n",
    "    persist_directory = './llm-universe/data_base/vector_db/chroma'\n",
    "    # 加载数据库\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "    return vectordb.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91dd1482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb079b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qa_history_chain():\n",
    "    retriever = get_retriever()\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "    condense_question_system_template = (\n",
    "        \"请根据聊天记录总结用户最近的问题，\"\n",
    "        \"如果没有多余的聊天记录则返回用户的问题。\"\n",
    "    )\n",
    "    condense_question_prompt = ChatPromptTemplate([\n",
    "            (\"system\", condense_question_system_template),\n",
    "            (\"placeholder\", \"{chat_history}\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "\n",
    "    retrieve_docs = RunnableBranch(\n",
    "        (lambda x: not x.get(\"chat_history\", False), (lambda x: x[\"input\"]) | retriever, ),\n",
    "        condense_question_prompt | llm | StrOutputParser() | retriever,\n",
    "    )\n",
    "\n",
    "    system_prompt = (\n",
    "        \"你是一个问答任务的助手。 \"\n",
    "        \"请使用检索到的上下文片段回答这个问题。 \"\n",
    "        \"如果你不知道答案就说不知道。 \"\n",
    "        \"请使用简洁的话语回答用户。\"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"placeholder\", \"{chat_history}\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    qa_chain = (\n",
    "        RunnablePassthrough().assign(context=combine_docs)\n",
    "        | qa_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    qa_history_chain = RunnablePassthrough().assign(\n",
    "        context = retrieve_docs, \n",
    "        ).assign(answer=qa_chain)\n",
    "    return qa_history_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c557ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_response(chain, input, chat_history):\n",
    "    response = chain.stream({\n",
    "        \"input\": input,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    for res in response:\n",
    "        if \"answer\" in res.keys():\n",
    "            yield res[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcb0c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.markdown('### 🦜🔗 动手学大模型应用开发')\n",
    "    # st.session_state可以存储用户与应用交互期间的状态与数据\n",
    "    # 存储对话历史\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "    # 存储检索问答链\n",
    "    if \"qa_history_chain\" not in st.session_state:\n",
    "        st.session_state.qa_history_chain = get_qa_history_chain()\n",
    "    # 建立容器 高度为500 px\n",
    "    messages = st.container(height=550)\n",
    "    # 显示整个对话历史\n",
    "    for message in st.session_state.messages: # 遍历对话历史\n",
    "            with messages.chat_message(message[0]): # messages指在容器下显示，chat_message显示用户及ai头像\n",
    "                st.write(message[1]) # 打印内容\n",
    "    if prompt := st.chat_input(\"Say something\"):\n",
    "        # 将用户输入添加到对话历史中\n",
    "        st.session_state.messages.append((\"human\", prompt))\n",
    "        # 显示当前用户输入\n",
    "        with messages.chat_message(\"human\"):\n",
    "            st.write(prompt)\n",
    "        # 生成回复\n",
    "        answer = gen_response(\n",
    "            chain=st.session_state.qa_history_chain,\n",
    "            input=prompt,\n",
    "            chat_history=st.session_state.messages\n",
    "        )\n",
    "        # 流式输出\n",
    "        with messages.chat_message(\"ai\"):\n",
    "            output = st.write_stream(answer)\n",
    "        # 将输出存入st.session_state.messages\n",
    "        st.session_state.messages.append((\"ai\", output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

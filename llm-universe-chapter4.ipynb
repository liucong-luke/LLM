{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04dd15d7",
   "metadata": {},
   "source": [
    "## æ„å»ºRAGåº”ç”¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511c0c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv \n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fefca3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7f6e416381c0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f6e2b46cfa0>, root_client=<openai.OpenAI object at 0x7f6e2b76ea70>, root_async_client=<openai.AsyncOpenAI object at 0x7f6e4163b730>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5467bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a893ca92-5ecf-42a8-a4b3-e34f0bec5ff1-0', usage_metadata={'input_tokens': 14, 'output_tokens': 7, 'total_tokens': 21})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm.invoke(\"What is the capital of France?\")\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4355b14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿™é‡Œæˆ‘ä»¬è¦æ±‚æ¨¡å‹å¯¹ç»™å®šæ–‡æœ¬è¿›è¡Œä¸­æ–‡ç¿»è¯‘\n",
    "prompt = \"\"\"è¯·ä½ å°†ç”±ä¸‰ä¸ªåå¼•å·åˆ†å‰²çš„æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼\\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03be79e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'è¯·ä½ å°†ç”±ä¸‰ä¸ªåå¼•å·åˆ†å‰²çš„æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡ï¼text: ```æˆ‘å¸¦ç€æ¯”èº«ä½“é‡çš„è¡Œæï¼Œæ¸¸å…¥å°¼ç½—æ²³åº•ï¼Œç»è¿‡å‡ é“é—ªç”µ çœ‹åˆ°ä¸€å †å…‰åœˆï¼Œä¸ç¡®å®šæ˜¯ä¸æ˜¯è¿™é‡Œã€‚```\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"æˆ‘å¸¦ç€æ¯”èº«ä½“é‡çš„è¡Œæï¼Œ\\\n",
    "æ¸¸å…¥å°¼ç½—æ²³åº•ï¼Œ\\\n",
    "ç»è¿‡å‡ é“é—ªç”µ çœ‹åˆ°ä¸€å †å…‰åœˆï¼Œ\\\n",
    "ä¸ç¡®å®šæ˜¯ä¸æ˜¯è¿™é‡Œã€‚\\\n",
    "\"\n",
    "prompt.format(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b610ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æˆ‘å°† ä¸­æ–‡ ç¿»è¯‘æˆ è‹±æ–‡.', additional_kwargs={}, response_metadata={}), HumanMessage(content='æˆ‘å¸¦ç€æ¯”èº«ä½“é‡çš„è¡Œæï¼Œæ¸¸å…¥å°¼ç½—æ²³åº•ï¼Œç»è¿‡å‡ é“é—ªç”µ çœ‹åˆ°ä¸€å †å…‰åœˆï¼Œä¸ç¡®å®šæ˜¯ä¸æ˜¯è¿™é‡Œã€‚', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"ä½ æ˜¯ä¸€ä¸ªç¿»è¯‘åŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©æˆ‘å°† {input_language} ç¿»è¯‘æˆ {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate([(\"system\", template), (\"human\", human_template)])\n",
    "\n",
    "text = \"æˆ‘å¸¦ç€æ¯”èº«ä½“é‡çš„è¡Œæï¼Œ\\\n",
    "æ¸¸å…¥å°¼ç½—æ²³åº•ï¼Œ\\\n",
    "ç»è¿‡å‡ é“é—ªç”µ çœ‹åˆ°ä¸€å †å…‰åœˆï¼Œ\\\n",
    "ä¸ç¡®å®šæ˜¯ä¸æ˜¯è¿™é‡Œã€‚\\\n",
    "\"\n",
    "messages = chat_prompt.invoke({\"input_language\": \"ä¸­æ–‡\", \"output_language\": \"è‹±æ–‡\", \"text\": text})\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a48b121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I carried luggage heavier than my body and dived into the bottom of the Nile River. After passing through several flashes of lightning, I saw a pile of halos, not sure if this is the place.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 42, 'prompt_tokens': 95, 'total_tokens': 137, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b17de4a1-685a-4028-909e-fdf226023916-0', usage_metadata={'input_tokens': 95, 'output_tokens': 42, 'total_tokens': 137})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = llm.invoke(messages)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "354b5b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I carried luggage heavier than my body and dived into the bottom of the Nile River. After passing through several flashes of lightning, I saw a pile of halos, not sure if this is the place.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "output_parser.invoke(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97834fb9",
   "metadata": {},
   "source": [
    "#### ç»„åˆæˆä¸€ä¸ªé“¾æ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ab751a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt | llm | output_parser # LCELæ¨¡æ¿è¯­æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa386de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'æˆ‘æ‰›ç€æ¯”æˆ‘çš„èº«ä½“è¿˜é‡çš„è¡Œæï¼Œæ½œå…¥äº†å°¼ç½—æ²³çš„åº•éƒ¨ã€‚ç©¿è¿‡å‡ é“é—ªç”µåï¼Œæˆ‘çœ‹åˆ°äº†ä¸€å †å…‰ç¯ï¼Œä¸ç¡®å®šè¿™æ˜¯å¦å°±æ˜¯ç›®çš„åœ°ã€‚'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'I carried luggage heavier than my body and dived into the bottom of the Nile River. After passing through several flashes of lightning, I saw a pile of halos, not sure if this is the place.'\n",
    "chain.invoke({\"input_language\": \"è‹±æ–‡\", \"output_language\": \"ä¸­æ–‡\",\"text\": text})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27cb918",
   "metadata": {},
   "source": [
    "### æ„å»ºæ£€ç´¢é—®ç­”é“¾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad4e24",
   "metadata": {},
   "source": [
    "#### åŠ è½½å‘é‡æ•°æ®åº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6225503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./llm-universe/notebook/C4 æ„å»º RAG åº”ç”¨\")\n",
    "\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64e1e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai_api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30866c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18932/3977479060.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  embeddings = OpenAIEmbeddings()\n",
      "/tmp/ipykernel_18932/3977479060.py:7: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  vector_db = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "persist_directory = './llm-universe/data_base/vector_db/chroma'\n",
    "\n",
    "vector_db = Chroma(\n",
    "  persist_directory=persist_directory,\n",
    "  embedding_function=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e37281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‘é‡åº“ä¸­å­˜å‚¨çš„æ•°é‡ï¼š0\n"
     ]
    }
   ],
   "source": [
    "print(f\"å‘é‡åº“ä¸­å­˜å‚¨çš„æ•°é‡ï¼š{vector_db._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22857436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ£€ç´¢åˆ°çš„å†…å®¹æ•°ï¼š0\n"
     ]
    }
   ],
   "source": [
    "question = \"ä»€ä¹ˆæ˜¯ prompt engineeringï¼Ÿ\"\n",
    "retriever = vector_db.as_retriever(search_keywords={'k': 3})\n",
    "docs = retriever.invoke(question)\n",
    "print(f\"æ£€ç´¢åˆ°çš„å†…å®¹æ•°ï¼š{len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a50b75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    print(f\"æ£€ç´¢åˆ°çš„ç¬¬{i}ä¸ªå†…å®¹: \\n {doc.page_content}\", end=\"\\n-----------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbeed0",
   "metadata": {},
   "source": [
    "#### åˆ›å»ºæ£€ç´¢é“¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24277ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def combine_docs(docs):\n",
    "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "combiner = RunnableLambda(combine_docs)\n",
    "retrieval_chain = retriever | combiner\n",
    "retrieval_chain.invoke(\"å—ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092dda96",
   "metadata": {},
   "source": [
    "#### åˆ›å»º LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7101b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b99e50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'å½“ç„¶å¯ä»¥ï¼æˆ‘æ˜¯ä¸€ä¸ªç”±OpenAIå¼€å‘çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œåå«ChatGPTã€‚æˆ‘æ“…é•¿å¤„ç†è‡ªç„¶è¯­è¨€ï¼Œå¯ä»¥å¸®åŠ©å›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€ååŠ©è§£å†³é—®é¢˜ä»¥åŠè¿›è¡Œå„ç§å¯¹è¯ã€‚æˆ‘æ²¡æœ‰ä¸ªäººæ„è¯†æˆ–æƒ…æ„Ÿï¼Œåªæ˜¯ä¸€ä¸ªåŸºäºå¤§é‡æ•°æ®è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨ä¸ºç”¨æˆ·æä¾›æœ‰ç”¨å’Œå‡†ç¡®çš„å›ç­”ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œéšæ—¶å¯ä»¥é—®æˆ‘ï¼'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0.0)\n",
    "\n",
    "llm.invoke('è¯·ä½ ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±').content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e3a974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”\n",
    "æ¡ˆã€‚æœ€å¤šä½¿ç”¨ä¸‰å¥è¯ã€‚å°½é‡ä½¿ç­”æ¡ˆç®€æ˜æ‰¼è¦ã€‚è¯·ä½ åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚\n",
    "{context}\n",
    "é—®é¢˜: {input}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template)\n",
    "\n",
    "qa_chain = (RunnableParallel({'context': retrieval_chain, 'input': RunnablePassthrough()}) | prompt | llm | StrOutputParser())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d145b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"ä»€ä¹ˆæ˜¯å—ç“œä¹¦ï¼Ÿ\"\n",
    "question_2 = \"Prompt Engineering for Developeræ˜¯è°å†™çš„ï¼Ÿ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec061a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤§æ¨¡å‹+çŸ¥è¯†åº“åå›ç­” question_1 çš„ç»“æœï¼š\n",
      "å—ç“œä¹¦æ˜¯å¯¹ã€Šæ·±åº¦å­¦ä¹ ã€‹è¿™æœ¬ä¹¦çš„æ˜µç§°ï¼Œç”±Ian Goodfellowã€Yoshua Bengioå’ŒAaron Courvilleåˆè‘—ã€‚ç”±äºä¹¦çš„å°é¢æ˜¯æ©™è‰²çš„ï¼Œå½¢ä¼¼å—ç“œï¼Œå› æ­¤å¾—åã€‚è¯¥ä¹¦æ˜¯æ·±åº¦å­¦ä¹ é¢†åŸŸçš„é‡è¦æ•™æã€‚è°¢è°¢ä½ çš„æé—®ï¼\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain.invoke(question_1)\n",
    "print(\"å¤§æ¨¡å‹+çŸ¥è¯†åº“åå›ç­” question_1 çš„ç»“æœï¼š\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad5d3818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¤§æ¨¡å‹+çŸ¥è¯†åº“åå›ç­” question_2 çš„ç»“æœï¼š\n",
      "æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘ä¸çŸ¥é“ã€ŠPrompt Engineering for Developerã€‹è¿™æœ¬ä¹¦æ˜¯è°å†™çš„ã€‚è°¢è°¢ä½ çš„æé—®ï¼\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain.invoke(question_2)\n",
    "print(\"å¤§æ¨¡å‹+çŸ¥è¯†åº“åå›ç­” question_2 çš„ç»“æœï¼š\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "068c4ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"å—ç“œä¹¦\"æ˜¯å¯¹ã€Šæ·±åº¦å­¦ä¹ ï¼šç®—æ³•ä¸å®ç°ã€‹çš„æ˜µç§°ï¼Œè¿™æœ¬ä¹¦ç”±ææ²ã€é˜¿æ–¯é¡¿å¼ ã€æ‰å¡é‡ŒÂ·CÂ·ç«‹é¡¿å’Œäºšå†å±±å¤§Â·JÂ·æ–¯è«æ‹‰ç¼–å†™ã€‚ä¹‹æ‰€ä»¥è¢«ç§°ä¸º\"å—ç“œä¹¦\"ï¼Œæ˜¯å› ä¸ºä¹¦çš„å°é¢ä¸Šæœ‰ä¸€ä¸ªæ˜¾çœ¼çš„å—ç“œå›¾æ¡ˆã€‚è¿™æœ¬ä¹¦ä»¥å…¶é€šä¿—æ˜“æ‡‚çš„è¯­è¨€å’Œä¸°å¯Œçš„ä»£ç ç¤ºä¾‹ï¼Œå¹¿å—æ·±åº¦å­¦ä¹ åˆå­¦è€…å’Œå®è·µè€…çš„æ¬¢è¿ã€‚ä¹¦ä¸­æ¶µç›–äº†æ·±åº¦å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µã€å¸¸ç”¨ç®—æ³•ä»¥åŠå¦‚ä½•ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶è¿›è¡Œå®ç°ã€‚'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(question_1).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14213675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ã€ŠPrompt Engineering for Developersã€‹æ˜¯ç”±Isa Fulfordå’ŒAndrew M. Whiteæ’°å†™çš„ã€‚è¿™æœ¬ä¹¦æ—¨åœ¨å¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°ç†è§£å’Œåº”ç”¨æç¤ºå·¥ç¨‹æŠ€æœ¯ï¼Œä»¥æé«˜ä¸å¤§å‹è¯­è¨€æ¨¡å‹çš„äº¤äº’æ•ˆæœã€‚'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(question_2).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e5364c",
   "metadata": {},
   "source": [
    "#### å‘å‘é‡æ·»åŠ èŠå¤©è®°å½•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89aea37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt = (\n",
    "    \"ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ \"\n",
    "    \"è¯·ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”è¿™ä¸ªé—®é¢˜ã€‚ \"\n",
    "    \"å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚ \"\n",
    "    \"è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate([('system', system_prompt), ('placeholder', '{chat_history}'), ('human', '{input}')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70a1e610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ è¯·ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”è¿™ä¸ªé—®é¢˜ã€‚ å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚ è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚\n",
      "\n",
      "\n",
      "å—ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "messages = qa_prompt.invoke({\n",
    "  'input': 'å—ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ',\n",
    "  'chat_history': [],\n",
    "  'context': ''\n",
    "})\n",
    "\n",
    "for message in messages.messages:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1338be9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ è¯·ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”è¿™ä¸ªé—®é¢˜ã€‚ å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚ è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚\n",
      "\n",
      "\n",
      "è¥¿ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ\n",
      "è¥¿ç“œä¹¦æ˜¯æŒ‡å‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ä¸€ä¹¦ï¼Œæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ã€‚\n",
      "ä½ å¯ä»¥ä»‹ç»ä¸€ä¸‹ä»–å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "messages = qa_prompt.invoke(\n",
    "    {\n",
    "        \"input\": \"ä½ å¯ä»¥ä»‹ç»ä¸€ä¸‹ä»–å—ï¼Ÿ\",\n",
    "        \"chat_history\": [\n",
    "            (\"human\", \"è¥¿ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ\"),\n",
    "            (\"ai\", \"è¥¿ç“œä¹¦æ˜¯æŒ‡å‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ä¸€ä¹¦ï¼Œæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ã€‚\"),\n",
    "        ],\n",
    "        \"context\": \"\"\n",
    "    }\n",
    ")\n",
    "for message in messages.messages:\n",
    "    print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4fa8b",
   "metadata": {},
   "source": [
    "#### å¸¦æœ‰ä¿¡æ¯å‹ç¼©çš„æ£€ç´¢é“¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f1d333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "condense_question_system_template = (\n",
    "    \"è¯·æ ¹æ®èŠå¤©è®°å½•å®Œå–„ç”¨æˆ·æœ€æ–°çš„é—®é¢˜ï¼Œ\"\n",
    "    \"å¦‚æœç”¨æˆ·æœ€æ–°çš„é—®é¢˜ä¸éœ€è¦å®Œå–„åˆ™è¿”å›ç”¨æˆ·çš„é—®é¢˜ã€‚\"\n",
    "    )\n",
    "\n",
    "condense_question_prompt = ChatPromptTemplate([\n",
    "  ('system', condense_question_system_template),\n",
    "  ('placeholder', '{chat_history}'),\n",
    "  ('human', '{input}')\n",
    "])\n",
    "\n",
    "retrieve_docs = RunnableBranch(\n",
    "    # åˆ†æ”¯ 1: è‹¥èŠå¤©è®°å½•ä¸­æ²¡æœ‰ chat_history åˆ™ç›´æ¥ä½¿ç”¨ç”¨æˆ·é—®é¢˜æŸ¥è¯¢å‘é‡æ•°æ®åº“\n",
    "    (lambda x: not x.get(\"chat_history\", False), (lambda x: x[\"input\"]) | retriever, ),\n",
    "    # åˆ†æ”¯ 2 : è‹¥èŠå¤©è®°å½•ä¸­æœ‰ chat_history åˆ™å…ˆè®© llm æ ¹æ®èŠå¤©è®°å½•å®Œå–„é—®é¢˜å†æŸ¥è¯¢å‘é‡æ•°æ®åº“\n",
    "    condense_question_prompt | llm | StrOutputParser() | retriever,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1084d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡æ–°å®šä¹‰ combine_docs\n",
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs[\"context\"]) # å°† docs æ”¹ä¸º docs[\"context\"]\n",
    "# å®šä¹‰é—®ç­”é“¾\n",
    "qa_chain = (\n",
    "    RunnablePassthrough.assign(context=combine_docs) # ä½¿ç”¨ combine_docs å‡½æ•°æ•´åˆ qa_prompt ä¸­çš„ context\n",
    "    | qa_prompt # é—®ç­”æ¨¡æ¿\n",
    "    | llm\n",
    "    | StrOutputParser() # è§„å®šè¾“å‡ºçš„æ ¼å¼ä¸º str\n",
    ")\n",
    "# å®šä¹‰å¸¦æœ‰å†å²è®°å½•çš„é—®ç­”é“¾\n",
    "qa_history_chain = RunnablePassthrough.assign(\n",
    "    context = (lambda x: x) | retrieve_docs # å°†æŸ¥è¯¢ç»“æœå­˜ä¸º content\n",
    "    ).assign(answer=qa_chain) # å°†æœ€ç»ˆç»“æœå­˜ä¸º answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c02b149a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'è¥¿ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ',\n",
       " 'chat_history': [],\n",
       " 'context': [],\n",
       " 'answer': '\"è¥¿ç“œä¹¦\"æ˜¯å¯¹ã€Šæœºå™¨å­¦ä¹ ã€‹ä¸€ä¹¦çš„æ˜µç§°ï¼Œç”±å‘¨å¿—åæ•™æˆç¼–å†™ã€‚ç”±äºä¹¦çš„å°é¢æ˜¯ç»¿è‰²çš„ï¼Œå¹¶ä¸”æœ‰ä¸€ä¸ªè¥¿ç“œçš„å›¾æ¡ˆï¼Œå› æ­¤è¢«ç§°ä¸º\"è¥¿ç“œä¹¦\"ã€‚è¿™æœ¬ä¹¦æ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ä¸€æœ¬é‡è¦æ•™æï¼Œå¹¿æ³›ç”¨äºæ•™å­¦å’Œè‡ªå­¦ã€‚'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¸å¸¦èŠå¤©è®°å½•\n",
    "qa_history_chain.invoke({\n",
    "    \"input\": \"è¥¿ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ\",\n",
    "    \"chat_history\": []\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab46e583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'å—ç“œä¹¦è·Ÿå®ƒæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ',\n",
       " 'chat_history': [('human', 'è¥¿ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ'),\n",
       "  ('ai', 'è¥¿ç“œä¹¦æ˜¯æŒ‡å‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ä¸€ä¹¦ï¼Œæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ã€‚')],\n",
       " 'context': [],\n",
       " 'answer': 'å—ç“œä¹¦æ˜¯æŒ‡æèˆªè€å¸ˆçš„ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹ä¸€ä¹¦ã€‚ä¸è¥¿ç“œä¹¦ä¸€æ ·ï¼Œå—ç“œä¹¦ä¹Ÿæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„é‡è¦æ•™æï¼Œä½†ä¾§é‡äºç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‚ä¸¤è€…éƒ½æ˜¯å­¦ä¹ æœºå™¨å­¦ä¹ çš„å¸¸ç”¨å‚è€ƒä¹¦ç±ã€‚'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# å¸¦èŠå¤©è®°å½•\n",
    "qa_history_chain.invoke({\n",
    "    \"input\": \"å—ç“œä¹¦è·Ÿå®ƒæœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ\",\n",
    "    \"chat_history\": [\n",
    "        (\"human\", \"è¥¿ç“œä¹¦æ˜¯ä»€ä¹ˆï¼Ÿ\"),\n",
    "        (\"ai\", \"è¥¿ç“œä¹¦æ˜¯æŒ‡å‘¨å¿—åè€å¸ˆçš„ã€Šæœºå™¨å­¦ä¹ ã€‹ä¸€ä¹¦ï¼Œæ˜¯æœºå™¨å­¦ä¹ é¢†åŸŸçš„ç»å…¸å…¥é—¨æ•™æä¹‹ä¸€ã€‚\"),\n",
    "    ]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb662d8f",
   "metadata": {},
   "source": [
    "### éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c012448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableBranch, RunnablePassthrough\n",
    "import sys\n",
    "sys.path.append(\"./llm-universe/notebook/C3 æ­å»ºçŸ¥è¯†åº“\") # å°†çˆ¶ç›®å½•æ”¾å…¥ç³»ç»Ÿè·¯å¾„ä¸­\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8eda5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retriever():\n",
    "    # å®šä¹‰ Embeddings\n",
    "    embedding = OpenAIEmbeddings()\n",
    "    # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„\n",
    "    persist_directory = './llm-universe/data_base/vector_db/chroma'\n",
    "    # åŠ è½½æ•°æ®åº“\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embedding\n",
    "    )\n",
    "    return vectordb.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91dd1482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs[\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb079b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qa_history_chain():\n",
    "    retriever = get_retriever()\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\n",
    "    condense_question_system_template = (\n",
    "        \"è¯·æ ¹æ®èŠå¤©è®°å½•æ€»ç»“ç”¨æˆ·æœ€è¿‘çš„é—®é¢˜ï¼Œ\"\n",
    "        \"å¦‚æœæ²¡æœ‰å¤šä½™çš„èŠå¤©è®°å½•åˆ™è¿”å›ç”¨æˆ·çš„é—®é¢˜ã€‚\"\n",
    "    )\n",
    "    condense_question_prompt = ChatPromptTemplate([\n",
    "            (\"system\", condense_question_system_template),\n",
    "            (\"placeholder\", \"{chat_history}\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ])\n",
    "\n",
    "    retrieve_docs = RunnableBranch(\n",
    "        (lambda x: not x.get(\"chat_history\", False), (lambda x: x[\"input\"]) | retriever, ),\n",
    "        condense_question_prompt | llm | StrOutputParser() | retriever,\n",
    "    )\n",
    "\n",
    "    system_prompt = (\n",
    "        \"ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚ \"\n",
    "        \"è¯·ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”è¿™ä¸ªé—®é¢˜ã€‚ \"\n",
    "        \"å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚ \"\n",
    "        \"è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚\"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"placeholder\", \"{chat_history}\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    qa_chain = (\n",
    "        RunnablePassthrough().assign(context=combine_docs)\n",
    "        | qa_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    qa_history_chain = RunnablePassthrough().assign(\n",
    "        context = retrieve_docs, \n",
    "        ).assign(answer=qa_chain)\n",
    "    return qa_history_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c557ba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_response(chain, input, chat_history):\n",
    "    response = chain.stream({\n",
    "        \"input\": input,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "    for res in response:\n",
    "        if \"answer\" in res.keys():\n",
    "            yield res[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dcb0c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    st.markdown('### ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘')\n",
    "    # st.session_stateå¯ä»¥å­˜å‚¨ç”¨æˆ·ä¸åº”ç”¨äº¤äº’æœŸé—´çš„çŠ¶æ€ä¸æ•°æ®\n",
    "    # å­˜å‚¨å¯¹è¯å†å²\n",
    "    if \"messages\" not in st.session_state:\n",
    "        st.session_state.messages = []\n",
    "    # å­˜å‚¨æ£€ç´¢é—®ç­”é“¾\n",
    "    if \"qa_history_chain\" not in st.session_state:\n",
    "        st.session_state.qa_history_chain = get_qa_history_chain()\n",
    "    # å»ºç«‹å®¹å™¨ é«˜åº¦ä¸º500 px\n",
    "    messages = st.container(height=550)\n",
    "    # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²\n",
    "    for message in st.session_state.messages: # éå†å¯¹è¯å†å²\n",
    "            with messages.chat_message(message[0]): # messagesæŒ‡åœ¨å®¹å™¨ä¸‹æ˜¾ç¤ºï¼Œchat_messageæ˜¾ç¤ºç”¨æˆ·åŠaiå¤´åƒ\n",
    "                st.write(message[1]) # æ‰“å°å†…å®¹\n",
    "    if prompt := st.chat_input(\"Say something\"):\n",
    "        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­\n",
    "        st.session_state.messages.append((\"human\", prompt))\n",
    "        # æ˜¾ç¤ºå½“å‰ç”¨æˆ·è¾“å…¥\n",
    "        with messages.chat_message(\"human\"):\n",
    "            st.write(prompt)\n",
    "        # ç”Ÿæˆå›å¤\n",
    "        answer = gen_response(\n",
    "            chain=st.session_state.qa_history_chain,\n",
    "            input=prompt,\n",
    "            chat_history=st.session_state.messages\n",
    "        )\n",
    "        # æµå¼è¾“å‡º\n",
    "        with messages.chat_message(\"ai\"):\n",
    "            output = st.write_stream(answer)\n",
    "        # å°†è¾“å‡ºå­˜å…¥st.session_state.messages\n",
    "        st.session_state.messages.append((\"ai\", output))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
